{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bbeelina/Project_1_DA/blob/main/lab1_gd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f46lxLSsBp0f"
      },
      "source": [
        "# Лабораторная работа 1. Градиентный спуск."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhdyseeXBp0k"
      },
      "source": [
        "В этой лабораторной работе вы напишете градиентный спуск для линейной регрессии, а так же посмотрите, как он ведёт себя с разными параметрами и разными функциями потерь."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do1I2ygaBp0k"
      },
      "source": [
        "Правила:\n",
        "\n",
        "* Лабораторная работа оценивается в 10 баллов.\n",
        "\n",
        "* Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания.\n",
        "\n",
        "* Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
        "\n",
        "* Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
        "\n",
        "* Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z8Nfs9riBp0k"
      },
      "outputs": [],
      "source": [
        "from typing import List, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j2vGNTyBp0l"
      },
      "source": [
        "Для начала давайте вспомним самый простой функционал ошибки, который мы применяем в задаче регрессии — Mean Squared Error:\n",
        "\n",
        "$$\n",
        "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2\n",
        "$$\n",
        "\n",
        "где $x_i$ — это $i$-ый объект датасета, $y_i$ — правильный ответ для $i$-го объекта, а $w$ — веса нашей линейной модели.\n",
        "\n",
        "Как мы помним, для линейной модели, его можно записать в матричном виде вот так:\n",
        "\n",
        "$$\n",
        "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2\n",
        "$$\n",
        "\n",
        "где $X$ — это матрица объекты-признаки, а $y$ — вектор правильных ответов\n",
        "\n",
        "Для того чтобы воспользоваться методом градиентного спуска, нам нужно посчитать градиент нашего функционала. Для MSE он будет выглядеть так:\n",
        "\n",
        "$$\n",
        "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05tP14CdBp0l"
      },
      "source": [
        "Ниже приведён базовый класс `BaseLoss`, который мы будем использовать для реализации всех наших лоссов. Менять его не нужно. У него есть два абстрактных метода:\n",
        "1. Метод `calc_loss`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения лосса\n",
        "2. Метод `calc_grad`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения градиента (вектор)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JWmOOrj6Bp0m"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "\n",
        "class BaseLoss(abc.ABC):\n",
        "    \"\"\"Базовый класс лосса\"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Функция для вычислений значения лосса\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: число -- значения функции потерь\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Функция для вычислений градиента лосса по весам w\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob65L_82Bp0m"
      },
      "source": [
        "Теперь давайте напишем реализацию этого абстрактоного класса: Mean Squared Error лосс."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezASiSCgBp0m"
      },
      "source": [
        "**Задание 1.1 (10/8 балла):** Реализуйте класс `MSELoss`\n",
        "\n",
        "Он должен вычислять лосс и градиент по формулам наверху"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gpvu5XryBp0m"
      },
      "outputs": [],
      "source": [
        "class MSELoss(BaseLoss):\n",
        "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Функция для вычислений значения лосса\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: число -- значения функции потерь\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
        "        loss = np.mean((X.dot(w) - y)**2)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Функция для вычислений градиента лосса по весам w\n",
        "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
        "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
        "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
        "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
        "        \"\"\"\n",
        "        # -- YOUR CODE HERE --\n",
        "        # Вычислите значение вектора градиента при помощи X, y и w и верните его)\n",
        "        gradient = (2 / len(y)) * X.T.dot(X.dot(w) - y)\n",
        "        return gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188svk-jBp0n"
      },
      "source": [
        "Теперь мы можем создать объект `MSELoss` и при помощи него вычислять значение нашей функции потерь и градиенты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yoQZnHVRBp0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce14064-c307-4c52-a145-4c710d195e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27410283.5\n",
            "[1163180. 1172281. 1181382. 1190483. 1199584. 1208685. 1217786. 1226887.\n",
            " 1235988. 1245089.]\n",
            "Всё верно!\n"
          ]
        }
      ],
      "source": [
        "# Создадим объект лосса\n",
        "loss = MSELoss()\n",
        "\n",
        "# Создадим какой-то датасет\n",
        "X = np.arange(200).reshape(20, 10)\n",
        "y = np.arange(20)\n",
        "\n",
        "# Создадим какой-то вектор весов\n",
        "w = np.arange(10)\n",
        "\n",
        "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
        "print(loss.calc_loss(X, y, w))\n",
        "print(loss.calc_grad(X, y, w))\n",
        "\n",
        "# Проверка, что методы реализованы правильно\n",
        "assert loss.calc_loss(X, y, w) == 27410283.5, \"Метод calc_loss реализован неверно\"\n",
        "assert np.allclose(loss.calc_grad(X, y, w), np.array([1163180., 1172281., 1181382., 1190483.,\n",
        "                                                      1199584., 1208685., 1217786., 1226887.,\n",
        "                                                      1235988., 1245089.])), \"Метод calc_grad реализован неверно\"\n",
        "print(\"Всё верно!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqRkUQpABp0n"
      },
      "source": [
        "Теперь когда у нас есть всё для вычисления градиента, давайте напишем наш градиентный спуск. Напомним, что формула для одной итерации градиентного спуска выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "w^t = w^{t-1} - \\eta \\nabla_{w} Q(w^{t-1}, X, y)\n",
        "$$\n",
        "\n",
        "Где $w^t$ — значение вектора весов на $t$-ой итерации, а $\\eta$ — параметр learning rate, отвечающий за размер шага."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9gOWGzqBp0n"
      },
      "source": [
        "**Задание 1.2 (10/8 балла):** Реализуйте функцию `gradient_descent`\n",
        "\n",
        "Функция должна принимать на вход начальное значение весов линейной модели `w_init`, матрицу объектов-признаков `X`,\n",
        "вектор правильных ответов `y`, объект функции потерь `loss`, размер шага `lr` и количество итераций `n_iterations`.\n",
        "\n",
        "Функция должна реализовывать цикл, в котором происходит шаг градиентного спуска (градиенты берутся из `loss` посредством вызова метода `calc_grad`) по формуле выше и возвращать\n",
        "траекторию спуска (список из новых значений весов на каждом шаге)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxu_8RHkBp0o"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray,\n",
        "                        loss: BaseLoss, lr: float, n_iterations: int = 100000) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Функция градиентного спуска\n",
        "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
        "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
        "    :param n_iterations: int -- сколько итераций делать\n",
        "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    \"\"\"\n",
        "    # -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM-gLeXPBp0o"
      },
      "source": [
        "Теперь создадим синтетический датасет и функцию, которая будет рисовать траекторию градиентного спуска по истории:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgaCBMiDBp0o"
      },
      "outputs": [],
      "source": [
        "# Создаём датасет из двух переменных и реального вектора зависимости w_true\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "n_features = 2\n",
        "n_objects = 300\n",
        "batch_size = 10\n",
        "num_steps = 43\n",
        "\n",
        "w_true = np.random.normal(size=(n_features, ))\n",
        "\n",
        "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
        "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
        "y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
        "w_init = np.random.uniform(-2, 2, (n_features))\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCsiY4PBBp0o"
      },
      "outputs": [],
      "source": [
        "loss = MSELoss()\n",
        "w_list = gradient_descent(w_init, X, y, loss, 0.01, 100)\n",
        "print(loss.calc_loss(X, y, w_list[0]))\n",
        "print(loss.calc_loss(X, y, w_list[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZm3EqMwBp0p"
      },
      "outputs": [],
      "source": [
        "def plot_gd(w_list: Iterable, X: np.ndarray, y: np.ndarray, loss: BaseLoss):\n",
        "    \"\"\"\n",
        "    Функция для отрисовки траектории градиентного спуска\n",
        "    :param w_list: Список из объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать лосс при помощи loss.calc_loss(X, y, w)\n",
        "    \"\"\"\n",
        "    w_list = np.array(w_list)\n",
        "    meshgrid_space = np.linspace(-2, 2, 100)\n",
        "    A, B = np.meshgrid(meshgrid_space, meshgrid_space)\n",
        "\n",
        "    levels = np.empty_like(A)\n",
        "    for i in range(A.shape[0]):\n",
        "        for j in range(A.shape[1]):\n",
        "            w_tmp = np.array([A[i, j], B[i, j]])\n",
        "            levels[i, j] = loss.calc_loss(X, y, w_tmp)\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.title(\"GD trajectory\")\n",
        "    plt.xlabel(r'$w_1$')\n",
        "    plt.ylabel(r'$w_2$')\n",
        "    plt.xlim(w_list[:, 0].min() - 0.1,\n",
        "             w_list[:, 0].max() + 0.1)\n",
        "    plt.ylim(w_list[:, 1].min() - 0.1,\n",
        "             w_list[:, 1].max() + 0.1)\n",
        "    plt.gca().set_aspect('equal')\n",
        "\n",
        "    # visualize the level set\n",
        "    CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
        "    CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "\n",
        "    # visualize trajectory\n",
        "    plt.scatter(w_list[:, 0], w_list[:, 1])\n",
        "    plt.plot(w_list[:, 0], w_list[:, 1])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqufHS3PBp0p"
      },
      "source": [
        "**Задание 1.3 (10/8 балла):** При помощи функций `gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`). Используйте не менее четырёх разных значений для `lr`.\n",
        "\n",
        "Сделайте и опишите свои выводы о том, как параметр `lr` влияет на поведение градиентного спуска\n",
        "\n",
        "Подсказки:\n",
        "* Функция `gradient_descent` возвращает историю весов, которую нужно подать в функцию `plot_gd`\n",
        "* Хорошие значения для `lr` могут лежать в промежутке от 0.0001 до 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPtpweFLBp0q"
      },
      "outputs": [],
      "source": [
        "# -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdxxpTlSBp0q"
      },
      "source": [
        "Теперь реализуем стохастический градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNXcI8AqBp0q"
      },
      "source": [
        "**Задание 1.4 (10/8 балла):** Реализуйте функцию `stochastic_gradient_descent`\n",
        "\n",
        "Функция должна принимать все те же параметры, что и функция `gradient_descent`, но ещё параметр `batch_size`, отвечающий за размер батча.\n",
        "\n",
        "Функция должна как и раньше реализовывать цикл, в котором происходит шаг градиентного спуска, но на каждом шаге считать градиент не по всей выборке `X`, а только по случайно выбранной части.\n",
        "\n",
        "Подсказка: для выбора случайной части можно использовать [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) с правильным параметром `size`, чтобы выбрать случайные индексы, а потом проиндексировать получившимся массивом массив `X`:\n",
        "```\n",
        "batch_indices = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
        "batch = X[batch_indices]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNlOVmKLBp0q"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray,\n",
        "                        loss: BaseLoss, lr: float, batch_size: int, n_iterations: int = 1000) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Функция градиентного спуска\n",
        "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
        "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
        "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
        "    :param n_iterations: int -- сколько итераций делать\n",
        "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    \"\"\"\n",
        "    # -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnQEuAFBp0r"
      },
      "source": [
        "**Задание 1.5 (10/8 балла):** При помощи функций `stochastic_gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`) и размера подвыборки (параметра `batch_size`). Используйте не менее четырёх разных значений для `lr` и `batch_size`.\n",
        "\n",
        "Сделайте и опишите свои выводы о том, как параметры  `lr` и `batch_size` влияют на поведение стохастического градиентного спуска. Как отличается поведение стохастического градиентного спуска от обычного?\n",
        "\n",
        "Обратите внимание, что в нашем датасете всего 300 объектов, так что `batch_size` больше этого числа не будет иметь смысла."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysPrEpkjBp0r"
      },
      "outputs": [],
      "source": [
        "# -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZWZE2VDBp0r"
      },
      "source": [
        "Вы могли заметить, что поведение градиентного спуска, особенно стохастической версии, очень сильно зависит от размера шага.\n",
        "\n",
        "Как правило, в начале спуска мы хотим делать большие шаги, чтобы поскорее подойти поближе к минимуму, а позже мы уже хотим делать шаги маленькие, чтобы более точнее этого минимума достичь и не \"перепрыгнуть\" его.\n",
        "\n",
        "Чтобы достичь такого поведения мы можем постепенно уменьшать длину шага с увеличением номера итерации. Сделать это можно, например, вычисляя на каждой итерации длину шага по следующей формуле:\n",
        "\n",
        "$$\n",
        "    \\eta_t\n",
        "    =\n",
        "    \\lambda\n",
        "    \\left(\n",
        "        \\frac{s_0}{s_0 + t}\n",
        "    \\right)^p\n",
        "$$\n",
        "\n",
        "где $\\eta_t$ — длина шага на итерации $t$, $\\lambda$ — начальная длина шага (параметр `lr` у нас), $s_0$ и $p$ — настраиваемые параметры."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hqj3zJ4Bp0r"
      },
      "source": [
        "**Задание 1.6 (10/8 балла):** Реализуйте функцию `stochastic_gradient_descent` на этот раз с затухающим шагом по формуле выше. Параметр $s_0$ возьмите равным 1. Параметр $p$ возьмите из нового аргумента функции `p`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uL65tOmBp0r"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(w_init: np.ndarray, X: np.ndarray, y: np.ndarray,\n",
        "                        loss: BaseLoss, lr: float, batch_size: int, p: float, n_iterations: int = 1000) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Функция градиентного спуска\n",
        "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
        "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
        "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
        "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
        "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
        "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
        "    :param p: float -- значение степени в формуле затухания длины шага\n",
        "    :param n_iterations: int -- сколько итераций делать\n",
        "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
        "    \"\"\"\n",
        "    # -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwgaqqB5Bp0r"
      },
      "source": [
        "**Задание 1.7 (10/8 балла):** При помощи новой функции `stochastic_gradient_descent` и функции `plot_gd` нарисуйте траекторию градиентного спуска для разных значений параметра `p`. Используйте не менее четырёх разных значений для `p`. Хорошими могут быть значения, лежащие в промежутке от 0.1 до 1.\n",
        "Параметр `lr` возьмите равным 0.01, а параметр `batch_size` равным 10.\n",
        "\n",
        "Сделайте и опишите свои выводы о том, как параметр `p` влияет на поведение стохастического градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri2bF6iCBp0s"
      },
      "outputs": [],
      "source": [
        "# -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0YKLC4mBp0s"
      },
      "source": [
        "**Задание 1.8 (10/8 балла):** Сравните сходимость обычного градиентного спуска и стохастичекой версии:\n",
        "Нарисуйте график зависимости значения лосса (его можно посчитать при помощи метода `calc_loss`, используя $x$ и $y$ из датасета и $w$ с соответствующей итерации) от номера итерации для траекторий, полученных при помощи обычного и стохастического градиентного спуска с одинаковыми параметрами. Параметр `batch_size` возьмите равным 10.\n",
        "\n",
        "Видно ли на данном графике преимущество SGD? Почему?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_f0qCQsBp0s"
      },
      "outputs": [],
      "source": [
        "# -- YOUR CODE HERE --"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdIqCkSEBp0w"
      },
      "source": [
        "**Задание 2 (0.08/8 балла)**\n",
        "Вставьте ваш любимый мем в ячейку ниже:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "numfffXPBp0x"
      },
      "outputs": [],
      "source": [
        "# -- YOUR MEME HERE --"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l5cGx9vyBp0x"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}